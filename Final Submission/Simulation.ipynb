{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1f3c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eec82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('ss_train_shock.csv')\n",
    "High = pd.read_csv('High_train.csv')\n",
    "Low = pd.read_csv('Low_train.csv')\n",
    "data = data.drop(data.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cfa313",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c99562",
   "metadata": {},
   "outputs": [],
   "source": [
    "High.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb97a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "Low.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b9383a",
   "metadata": {},
   "source": [
    "Whole Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d1199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ec4a32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fitter import Fitter, get_common_distributions, get_distributions\n",
    "def find_best_distribution(data, variable_name):\n",
    "\n",
    "    variable_data = data[variable_name].dropna()\n",
    "    f = Fitter(variable_data, distributions=get_common_distributions())\n",
    "    f.fit()\n",
    "    f.summary()\n",
    "    return f\n",
    "\n",
    "variable_names = ['HEALTH_CHANGE', 'HBP', 'DIABETES', 'CANCER',\n",
    "       'LUNGS', 'HEART_ATTACK', 'STROKE', 'PSYCH', 'ARTHRITIS', 'OUT_PT',\n",
    "       'DRUGS', 'HOME_CARE', 'SPECIAL_FAC', 'HOSPITAL', 'DOCTOR',\n",
    "       'NURSING_HOME']\n",
    "\n",
    "distribution_params = {}\n",
    "for variable in variable_names:\n",
    "    print(f\"Analyzing {variable}\")\n",
    "    dist = find_best_distribution(data, variable)\n",
    "    distribution_params[variable] = dist.get_best(method='sumsquare_error')\n",
    "    print(dist.get_best(method='sumsquare_error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3f5026",
   "metadata": {},
   "outputs": [],
   "source": [
    "shock_data = data[data['SHOCK2']==1]\n",
    "no_shock_data = data[data['SHOCK2']==0]\n",
    "variable_names = ['SPEND_SS']\n",
    "spending = [shock_data, no_shock_data]\n",
    "spending_results = {}\n",
    "for i in range(2):\n",
    "    spending_results[i] = find_best_distribution(spending[i], 'SPEND_SS')\n",
    "    print(spending_results[i].get_best(method='sumsquare_error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cba7745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def modelling(data, predictors, response):\n",
    "    # Create a lagged version of 'SHOCK2' to use as the target variable\n",
    "    data['response_next'] = data.groupby('HHIDPN')[response].shift(-1)\n",
    "    \n",
    "    \n",
    "    data_clean = data.dropna(subset=predictors + ['response_next'])\n",
    "    \n",
    "    X = data_clean[predictors]\n",
    "    y = data_clean['response_next']\n",
    "    #class_weights = {0: 1, 1: 13}\n",
    "    model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1127e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "train = pd.read_csv('ss_train_shock.csv')\n",
    "test = pd.read_csv('ss_test_shock.csv')\n",
    "def modelling_linear(data, data_test, predictors, response):\n",
    "    data = data[data['AGE']<=90]\n",
    "    data_test = data_test[data_test['AGE']<=90]\n",
    "    data['response_next'] = data.groupby('HHIDPN')[response].shift(-1)\n",
    "    data_test['response_next'] = data_test.groupby('HHIDPN')[response].shift(-1)\n",
    "    data_clean = data.dropna(subset=predictors + ['response_next'])\n",
    "    \n",
    "    X_train = data_clean[predictors]\n",
    "    y_train = data_clean['response_next']\n",
    "    X_test = data_test[predictors]\n",
    "    y_test = data_test['response_next']\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate residuals\n",
    "    residuals = y_test - y_pred\n",
    "    mean_residuals = np.mean(residuals)\n",
    "    std_residuals = np.std(residuals)\n",
    "\n",
    "    return model, mean_residuals, std_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20205219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def modelling_linear(data, predictors, response):\n",
    "#     data['response_next'] = data.groupby('HHIDPN')[response].shift(-1)\n",
    "#     data_clean = data.dropna(subset=predictors + ['response_next'])\n",
    "    \n",
    "#     X = data_clean[predictors]\n",
    "#     y = data_clean['response_next']\n",
    "    \n",
    "#     pipeline = Pipeline([\n",
    "#         ('scaler', StandardScaler()),\n",
    "#         ('model', LinearRegression())\n",
    "#     ])\n",
    "    \n",
    "#     pipeline.fit(X, y)\n",
    "    \n",
    "#     return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff11e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['AGE', 'HEALTH_CHANGE', 'HBP', 'STROKE', 'ARTHRITIS', 'DRUGS', 'SPECIAL_FAC', 'HOSPITAL', 'DOCTOR', 'NURSING_HOME']\n",
    "response = 'SHOCK2'\n",
    "shock_model = modelling(data, predictors, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f821e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_pred = ['AGE', 'PSYCH', 'DRUGS', 'HOME_CARE', 'HOSPITAL', 'DOCTOR', 'NURSING_HOME']\n",
    "spend_response = 'SPEND_SS'\n",
    "yes_model, yes_mean, yes_resid= modelling_linear(train, test, yes_pred, spend_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8a17bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_pred = ['AGE', 'HEALTH_CHANGE', 'DRUGS', 'HOME_CARE', 'DOCTOR', 'NURSING_HOME']\n",
    "no_model, no_mean, no_resid = modelling_linear(train, test, no_pred, spend_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0a8458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def simulate_individual_data(num_individuals, start_age, end_age, distribution_params):\n",
    "    simulated_complete_data = pd.DataFrame()\n",
    "\n",
    "    # Simulate data for each individual\n",
    "    for person_id in range(1, num_individuals + 1):\n",
    "        # Initialize a DataFrame for the current individual's data across the specified age range\n",
    "        individual_data = pd.DataFrame({\n",
    "            'AGE': range(start_age, end_age + 1),\n",
    "            'HHIDPN': person_id  # Assign the unique person ID to each row\n",
    "        })\n",
    "\n",
    "        # Simulate the data for each variable according to the distribution parameters\n",
    "        for variable, params_dict in distribution_params.items():\n",
    "            dist_name, params = next(iter(params_dict.items()))  # Get the distribution and its parameters\n",
    "            dist = getattr(stats, dist_name)  \n",
    "            individual_data[variable] = dist.rvs(size=end_age - start_age + 1, **params)\n",
    "        \n",
    "        simulated_complete_data = pd.concat([simulated_complete_data, individual_data], ignore_index=True)\n",
    "\n",
    "    return simulated_complete_data\n",
    "\n",
    "# Example usage\n",
    "num_individuals = 10000\n",
    "\n",
    "stacked_simulation = simulate_individual_data(num_individuals, 65, 90, distribution_params)\n",
    "stacked_simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403a79ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def add_shock_predictions(simulated_data, shock_model, predictors):\n",
    "    \n",
    "    predicted_shocks = shock_model.predict(simulated_data[predictors])\n",
    "    \n",
    "    simulated_data['SHOCK2'] = predicted_shocks\n",
    "    \n",
    "    return simulated_data\n",
    "\n",
    "simulated_data_with_shock = add_shock_predictions(stacked_simulation, shock_model, predictors)\n",
    "\n",
    "simulated_data_with_shock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8385499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spend_ss_predictions(simulated_data, yes_model, no_model, yes_pred, no_pred):\n",
    "    simulated_data['SPEND_SS'] = 0.0\n",
    "\n",
    "    shock_indices = simulated_data[simulated_data['SHOCK2'] == 1].index\n",
    "    yes_predictions = yes_model.predict(simulated_data.loc[shock_indices, yes_pred])\n",
    "    yes_random_residuals = np.random.normal(loc=yes_mean, scale=yes_resid, size=len(shock_indices))\n",
    "    simulated_data.loc[shock_indices, 'SPEND_SS'] = yes_predictions - yes_random_residuals\n",
    "\n",
    "    no_shock_indices = simulated_data[simulated_data['SHOCK2'] == 0].index\n",
    "    no_predictions = no_model.predict(simulated_data.loc[no_shock_indices, no_pred])\n",
    "    no_random_residuals = np.random.normal(loc=no_mean, scale=no_resid, size=len(no_shock_indices))\n",
    "    simulated_data.loc[no_shock_indices, 'SPEND_SS'] = no_predictions - no_random_residuals\n",
    "    \n",
    "    return simulated_data\n",
    "\n",
    "# Example usage\n",
    "simulated_data_final = add_spend_ss_predictions(simulated_data_with_shock, yes_model, no_model, yes_pred, no_pred)\n",
    "simulated_data_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f942d61d",
   "metadata": {},
   "source": [
    "High Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77289039",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('High_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc76403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fitter import Fitter, get_common_distributions, get_distributions\n",
    "def find_best_distribution(data, variable_name):\n",
    "\n",
    "    variable_data = data[variable_name].dropna()\n",
    "    f = Fitter(variable_data, distributions=get_common_distributions())\n",
    "    f.fit()\n",
    "    f.summary()\n",
    "    return f\n",
    "\n",
    "variable_names = ['HEALTH_CHANGE', 'HBP', 'DIABETES', 'CANCER',\n",
    "       'LUNGS', 'HEART_ATTACK', 'STROKE', 'PSYCH', 'ARTHRITIS', 'OUT_PT',\n",
    "       'DRUGS', 'HOME_CARE', 'SPECIAL_FAC', 'HOSPITAL', 'DOCTOR',\n",
    "       'NURSING_HOME']\n",
    "\n",
    "distribution_params = {}\n",
    "for variable in variable_names:\n",
    "    print(f\"Analyzing {variable}\")\n",
    "    dist = find_best_distribution(data, variable)\n",
    "    distribution_params[variable] = dist.get_best(method='sumsquare_error')\n",
    "    print(dist.get_best(method='sumsquare_error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3edcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "shock_data = data[data['SHOCK2']==1]\n",
    "no_shock_data = data[data['SHOCK2']==0]\n",
    "variable_names = ['SPEND_SS']\n",
    "spending = [shock_data, no_shock_data]\n",
    "spending_results = {}\n",
    "for i in range(2):\n",
    "    spending_results[i] = find_best_distribution(spending[i], 'SPEND_SS')\n",
    "    print(spending_results[i].get_best(method='sumsquare_error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2f44e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def modelling(data, predictors, response):\n",
    "    # Create a lagged version of 'SHOCK2' to use as the target variable\n",
    "    data['response_next'] = data.groupby('HHIDPN')[response].shift(-1)\n",
    "    \n",
    "    \n",
    "    data_clean = data.dropna(subset=predictors + ['response_next'])\n",
    "    \n",
    "    X = data_clean[predictors]\n",
    "    y = data_clean['response_next']\n",
    "    #class_weights = {0: 1, 1: 13}\n",
    "    model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95bb4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "train = pd.read_csv('ss_train_shock.csv')\n",
    "test = pd.read_csv('ss_test_shock.csv')\n",
    "def modelling_linear(data, data_test, predictors, response):\n",
    "    data = data[data['AGE']<=90]\n",
    "    data_test = data_test[data_test['AGE']<=90]\n",
    "    data['response_next'] = data.groupby('HHIDPN')[response].shift(-1)\n",
    "    data_test['response_next'] = data_test.groupby('HHIDPN')[response].shift(-1)\n",
    "    data_clean = data.dropna(subset=predictors + ['response_next'])\n",
    "    \n",
    "    X_train = data_clean[predictors]\n",
    "    y_train = data_clean['response_next']\n",
    "    X_test = data_test[predictors]\n",
    "    y_test = data_test['response_next']\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate residuals\n",
    "    residuals = y_test - y_pred\n",
    "    mean_residuals = np.mean(residuals)\n",
    "    std_residuals = np.std(residuals)\n",
    "\n",
    "    return model, mean_residuals, std_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2b6857",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['AGE', 'HEALTH_CHANGE', 'HBP', 'STROKE', 'ARTHRITIS', 'DRUGS', 'SPECIAL_FAC', 'HOSPITAL', 'DOCTOR', 'NURSING_HOME']\n",
    "response = 'SHOCK2'\n",
    "shock_model = modelling(data, predictors, response)\n",
    "\n",
    "yes_pred = ['AGE', 'PSYCH', 'DRUGS', 'HOME_CARE', 'HOSPITAL', 'DOCTOR', 'NURSING_HOME']\n",
    "spend_response = 'SPEND_SS'\n",
    "yes_model, yes_mean, yes_resid = modelling_linear(train, test, yes_pred, spend_response)\n",
    "\n",
    "no_pred = ['AGE', 'HEALTH_CHANGE', 'DRUGS', 'HOME_CARE', 'DOCTOR', 'NURSING_HOME']\n",
    "no_model, no_mean, no_resid = modelling_linear(train, test, no_pred, spend_response)\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "def simulate_individual_data(num_individuals, start_age, end_age, distribution_params):\n",
    "    simulated_complete_data = pd.DataFrame()\n",
    "\n",
    "    # Simulate data for each individual\n",
    "    for person_id in range(1, num_individuals + 1):\n",
    "        # Initialize a DataFrame for the current individual's data across the specified age range\n",
    "        individual_data = pd.DataFrame({\n",
    "            'AGE': range(start_age, end_age + 1),\n",
    "            'HHIDPN': person_id  # Assign the unique person ID to each row\n",
    "        })\n",
    "\n",
    "        # Simulate the data for each variable according to the distribution parameters\n",
    "        for variable, params_dict in distribution_params.items():\n",
    "            dist_name, params = next(iter(params_dict.items()))  # Get the distribution and its parameters\n",
    "            dist = getattr(stats, dist_name)  \n",
    "            individual_data[variable] = dist.rvs(size=end_age - start_age + 1, **params)\n",
    "        \n",
    "        simulated_complete_data = pd.concat([simulated_complete_data, individual_data], ignore_index=True)\n",
    "\n",
    "    return simulated_complete_data\n",
    "\n",
    "# Example usage\n",
    "num_individuals = 10000\n",
    "\n",
    "stacked_simulation = simulate_individual_data(num_individuals, 65, 90, distribution_params)\n",
    "stacked_simulation\n",
    "\n",
    "def add_shock_predictions(simulated_data, shock_model, predictors):\n",
    "    \n",
    "    predicted_shocks = shock_model.predict(simulated_data[predictors])\n",
    "    \n",
    "    simulated_data['SHOCK2'] = predicted_shocks\n",
    "    \n",
    "    return simulated_data\n",
    "\n",
    "simulated_data_with_shock = add_shock_predictions(stacked_simulation, shock_model, predictors)\n",
    "\n",
    "simulated_data_with_shock\n",
    "\n",
    "def add_spend_ss_predictions(simulated_data, yes_model, no_model, yes_pred, no_pred):\n",
    "    simulated_data['SPEND_SS'] = 0.0\n",
    "\n",
    "    shock_indices = simulated_data[simulated_data['SHOCK2'] == 1].index\n",
    "    yes_predictions = yes_model.predict(simulated_data.loc[shock_indices, yes_pred])\n",
    "    yes_random_residuals = np.random.normal(loc=yes_mean, scale=yes_resid, size=len(shock_indices))\n",
    "    simulated_data.loc[shock_indices, 'SPEND_SS'] = yes_predictions - yes_random_residuals\n",
    "\n",
    "    no_shock_indices = simulated_data[simulated_data['SHOCK2'] == 0].index\n",
    "    no_predictions = no_model.predict(simulated_data.loc[no_shock_indices, no_pred])\n",
    "    no_random_residuals = np.random.normal(loc=no_mean, scale=no_resid, size=len(no_shock_indices))\n",
    "    simulated_data.loc[no_shock_indices, 'SPEND_SS'] = no_predictions - no_random_residuals\n",
    "    \n",
    "    return simulated_data\n",
    "\n",
    "# Example usage\n",
    "simulated_data_final_high = add_spend_ss_predictions(simulated_data_with_shock, yes_model, no_model, yes_pred, no_pred)\n",
    "simulated_data_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30103def",
   "metadata": {},
   "source": [
    "Low Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73ece85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Low_train.csv')\n",
    "\n",
    "from fitter import Fitter, get_common_distributions, get_distributions\n",
    "def find_best_distribution(data, variable_name):\n",
    "\n",
    "    variable_data = data[variable_name].dropna()\n",
    "    f = Fitter(variable_data, distributions=get_common_distributions())\n",
    "    f.fit()\n",
    "    f.summary()\n",
    "    return f\n",
    "\n",
    "variable_names = ['HEALTH_CHANGE', 'HBP', 'DIABETES', 'CANCER',\n",
    "       'LUNGS', 'HEART_ATTACK', 'STROKE', 'PSYCH', 'ARTHRITIS', 'OUT_PT',\n",
    "       'DRUGS', 'HOME_CARE', 'SPECIAL_FAC', 'HOSPITAL', 'DOCTOR',\n",
    "       'NURSING_HOME']\n",
    "\n",
    "distribution_params = {}\n",
    "for variable in variable_names:\n",
    "    print(f\"Analyzing {variable}\")\n",
    "    dist = find_best_distribution(data, variable)\n",
    "    distribution_params[variable] = dist.get_best(method='sumsquare_error')\n",
    "    print(dist.get_best(method='sumsquare_error'))\n",
    "\n",
    "shock_data = data[data['SHOCK2']==1]\n",
    "no_shock_data = data[data['SHOCK2']==0]\n",
    "variable_names = ['SPEND_SS']\n",
    "spending = [shock_data, no_shock_data]\n",
    "spending_results = {}\n",
    "for i in range(2):\n",
    "    spending_results[i] = find_best_distribution(spending[i], 'SPEND_SS')\n",
    "    print(spending_results[i].get_best(method='sumsquare_error'))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def modelling(data, predictors, response):\n",
    "    # Create a lagged version of 'SHOCK2' to use as the target variable\n",
    "    data['response_next'] = data.groupby('HHIDPN')[response].shift(-1)\n",
    "    \n",
    "    \n",
    "    data_clean = data.dropna(subset=predictors + ['response_next'])\n",
    "    \n",
    "    X = data_clean[predictors]\n",
    "    y = data_clean['response_next']\n",
    "    #class_weights = {0: 1, 1: 13}\n",
    "    model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    return model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "train = pd.read_csv('ss_train_shock.csv')\n",
    "test = pd.read_csv('ss_test_shock.csv')\n",
    "def modelling_linear(data, data_test, predictors, response):\n",
    "    data = data[data['AGE']<=90]\n",
    "    data_test = data_test[data_test['AGE']<=90]\n",
    "    data['response_next'] = data.groupby('HHIDPN')[response].shift(-1)\n",
    "    data_test['response_next'] = data_test.groupby('HHIDPN')[response].shift(-1)\n",
    "    data_clean = data.dropna(subset=predictors + ['response_next'])\n",
    "    \n",
    "    X_train = data_clean[predictors]\n",
    "    y_train = data_clean['response_next']\n",
    "    X_test = data_test[predictors]\n",
    "    y_test = data_test['response_next']\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate residuals\n",
    "    residuals = y_test - y_pred\n",
    "    mean_residuals = np.mean(residuals)\n",
    "    std_residuals = np.std(residuals)\n",
    "\n",
    "    return model, mean_residuals, std_residuals\n",
    "\n",
    "predictors = ['AGE', 'HEALTH_CHANGE', 'HBP', 'STROKE', 'ARTHRITIS', 'DRUGS', 'SPECIAL_FAC', 'HOSPITAL', 'DOCTOR', 'NURSING_HOME']\n",
    "response = 'SHOCK2'\n",
    "shock_model = modelling(data, predictors, response)\n",
    "\n",
    "yes_pred = ['AGE', 'PSYCH', 'DRUGS', 'HOME_CARE', 'HOSPITAL', 'DOCTOR', 'NURSING_HOME', 'CANCER']\n",
    "spend_response = 'SPEND_SS'\n",
    "yes_model, yes_mean, yes_resid = modelling_linear(train, test, yes_pred, spend_response)\n",
    "\n",
    "no_pred = ['AGE', 'HEALTH_CHANGE', 'DRUGS', 'HOME_CARE', 'DOCTOR', 'NURSING_HOME', 'CANCER']\n",
    "no_model, no_mean, no_resid = modelling_linear(train, test, no_pred, spend_response)\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "def simulate_individual_data(num_individuals, start_age, end_age, distribution_params):\n",
    "    simulated_complete_data = pd.DataFrame()\n",
    "\n",
    "    # Simulate data for each individual\n",
    "    for person_id in range(1, num_individuals + 1):\n",
    "        # Initialize a DataFrame for the current individual's data across the specified age range\n",
    "        individual_data = pd.DataFrame({\n",
    "            'AGE': range(start_age, end_age + 1),\n",
    "            'HHIDPN': person_id  # Assign the unique person ID to each row\n",
    "        })\n",
    "\n",
    "        # Simulate the data for each variable according to the distribution parameters\n",
    "        for variable, params_dict in distribution_params.items():\n",
    "            dist_name, params = next(iter(params_dict.items()))  # Get the distribution and its parameters\n",
    "            dist = getattr(stats, dist_name)  \n",
    "            individual_data[variable] = dist.rvs(size=end_age - start_age + 1, **params)\n",
    "        \n",
    "        simulated_complete_data = pd.concat([simulated_complete_data, individual_data], ignore_index=True)\n",
    "\n",
    "    return simulated_complete_data\n",
    "\n",
    "# Example usage\n",
    "num_individuals = 10000\n",
    "\n",
    "stacked_simulation = simulate_individual_data(num_individuals, 65, 90, distribution_params)\n",
    "stacked_simulation\n",
    "\n",
    "def add_shock_predictions(simulated_data, shock_model, predictors):\n",
    "    \n",
    "    predicted_shocks = shock_model.predict(simulated_data[predictors])\n",
    "    \n",
    "    simulated_data['SHOCK2'] = predicted_shocks\n",
    "    \n",
    "    return simulated_data\n",
    "\n",
    "simulated_data_with_shock = add_shock_predictions(stacked_simulation, shock_model, predictors)\n",
    "\n",
    "simulated_data_with_shock\n",
    "\n",
    "def add_spend_ss_predictions(simulated_data, yes_model, no_model, yes_pred, no_pred):\n",
    "    simulated_data['SPEND_SS'] = 0.0\n",
    "\n",
    "    shock_indices = simulated_data[simulated_data['SHOCK2'] == 1].index\n",
    "    yes_predictions = yes_model.predict(simulated_data.loc[shock_indices, yes_pred])\n",
    "    yes_random_residuals = np.random.normal(loc=yes_mean, scale=yes_resid, size=len(shock_indices))\n",
    "    simulated_data.loc[shock_indices, 'SPEND_SS'] = yes_predictions - yes_random_residuals\n",
    "\n",
    "    no_shock_indices = simulated_data[simulated_data['SHOCK2'] == 0].index\n",
    "    no_predictions = no_model.predict(simulated_data.loc[no_shock_indices, no_pred])\n",
    "    no_random_residuals = np.random.normal(loc=no_mean, scale=no_resid, size=len(no_shock_indices))\n",
    "    simulated_data.loc[no_shock_indices, 'SPEND_SS'] = no_predictions - no_random_residuals\n",
    "    \n",
    "    return simulated_data\n",
    "\n",
    "# Example usage\n",
    "simulated_data_final_low = add_spend_ss_predictions(simulated_data_with_shock, yes_model, no_model, yes_pred, no_pred)\n",
    "simulated_data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c1f0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('ss_train_shock.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ce404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_average_spending_by_age(simulated_data, simulated_data_low, simulated_data_high):\n",
    "    average_spending_by_age = simulated_data.groupby('AGE')['SPEND_SS'].mean()\n",
    "    average_spending_by_age_low = simulated_data_low.groupby('AGE')['SPEND_SS'].mean()\n",
    "    average_spending_by_age_high = simulated_data_high.groupby('AGE')['SPEND_SS'].mean()\n",
    "    sample_spending = data[data['AGE']<=90].groupby('AGE')['SPEND_SS'].mean()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    average_spending_by_age.plot(kind='line', marker='x', label='General Simulated Spending',color='blue')\n",
    "    average_spending_by_age_low.plot(kind='line', marker='x', label='Simulated Spending - Low Cluster',color='red')\n",
    "    average_spending_by_age_high.plot(kind='line', marker='x', label='Simulated Spending - High Cluster',color='green')\n",
    "    sample_spending.plot(kind='line', marker='x', label='Sample Spending',color='black')\n",
    "    plt.title('Average Spending by Age')\n",
    "    plt.xlabel('Age')\n",
    "    plt.ylabel('Average Spending')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage with the simulated data\n",
    "plot_average_spending_by_age(simulated_data_final, simulated_data_final_low, simulated_data_final_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3b946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_median_spending_by_age(simulated_data, simulated_data_low, simulated_data_high):\n",
    "    average_spending_by_age = simulated_data.groupby('AGE')['SPEND_SS'].median()\n",
    "    average_spending_by_age_low = simulated_data_low.groupby('AGE')['SPEND_SS'].median()\n",
    "    average_spending_by_age_high = simulated_data_high.groupby('AGE')['SPEND_SS'].median()\n",
    "    sample_spending = data[data['AGE']<=90].groupby('AGE')['SPEND_SS'].median()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    average_spending_by_age.plot(kind='line', marker='x', label='General Simulated Spending',color='blue')\n",
    "    average_spending_by_age_low.plot(kind='line', marker='x', label='Simulated Spending - Low Cluster',color='red')\n",
    "    average_spending_by_age_high.plot(kind='line', marker='x', label='Simulated Spending - High Cluster',color='green')\n",
    "    sample_spending.plot(kind='line', marker='x', label='Sample Spending',color='black')\n",
    "    plt.title('Median Spending by Age')\n",
    "    plt.xlabel('Age')\n",
    "    plt.ylabel('Median Spending')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage with the simulated data\n",
    "plot_median_spending_by_age(simulated_data_final, simulated_data_final_low, simulated_data_final_high)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
